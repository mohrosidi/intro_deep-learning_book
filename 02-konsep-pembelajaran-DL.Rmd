# Konsep Proses Pembelajaran *Deep Learning* {#DL}

<style>
body{
text-align: justify}
</style>

Proses pembelajaran *Artificial Neural Network* (ANN) pada dasarnya digunakan untuk mencari nilai bobot ($w$) optimal yang menghubungkan antar lapisan neuron sehingga diperoleh *cost* atau kesalahan minimum. Proses pembelajaran pada ANN dilakukan melalui dua proses utama, antara lain:

1. Propagasi maju (*forward propagation*)
2. Propagasi mundur (*bacward propagation*)

Secara umum propagasi maju proses dimana ANN memproses input data untuk melakukan prediksi terhadap nilai variabel target. Tahapan ini melibatkan proses perkalian titik antara input dan bobot dan pemjumlahan dengan bias dari masing-masing *layer* ANN. Hasil penjumlahan selanjutnya transformasi melalui fungsi aktivasi. 

Pada awal proses pembelajaran, bobot dat bias pada tiap jaringan ditetapkan secara acak, sehingga jaringan mengimplementasikan serangkaian transformasi secara acak. Hal ini menyebabkan nilai output yang dihasilkan jauh dari nilai yang diharapkan atau *cost* yang dihasilkan sangat tinggi. Untuk mengatasi *cost* yang tinggi tersebut, bobot dilakukan penyesuaian sehingga pada proses pembelajaran selanjutnya nilai *cost* yang dihasilkan menurun. Proses tersebut diulangi secara terus-menerus hingga diperoleh *cost* minimum yang konvergen. Tahapan tersebut disebut sebagai tahapan propagasi balik. Pada tahapan ini, hasil prediksi dari proses propagasi balik dihitung nilai *cost*-nya (error). Nilai tersebut selanjutnya dijadikan sebagai dasar dalam melakukan penyesuaian bobot dan bias pada ANN.

Gambaran proses pembelajaran ditampilkan pada Gambar \@ref(fig:learndl).

```{r learndl,echo=FALSE, fig.cap='Proses pembelajaran ANN. (Sumber:Chollet, 2018)', tidy=FALSE, out.width='80%', fig.align='center', message=FALSE, warning=FALSE}
library(knitr)
img1_path <- "./images/learndl.png"
include_graphics(img1_path)
```

## Notasi Matematika {#notation}

Terdapat sejumlah notasi matematika yang akan digunakan dalam pembahasan Chapter ini. Notasi matematika tersebut antara lain:

1. Untuk input $x \in R^{n_x}$ ($n$ variabel atau fitur)

* superskrip ($i$): menunjukkan referensi observasi ke-$i$, contoh: $x^{\left(2\right)}$ merupakan nilai input pada observasi atau baris kedua.
* subskrip $n$ : menunjukkan referensi fitur atau variabel ke-$n$, contoh: $x^{\left(2\right)}_{1}$ merupakan observasi kedua pada fitur pertama.

2. Untuk *network layer*:

* superskrip [$I$]: menunjukkan referensi *layer* ke-$I$, contoh: $z^{\left[2\right]}$ merupakan fungsi aktivasi pada *layer* 2.
* subskrip $n$ : menunjukkan referensi node atau neuron ke-$n$, contoh: $z^{\left[2\right]}_{1}$ merupakan fungsi aktivasi ditambah bias pada node 1 pada *layer* 2.

## Konsep Propagasi Maju {#forwardprop}

<style>
body{
text-align: justify}
</style>

Input sebuah model ANN dapat berupa apapun. Input dapat berupa intensitas abu-abu (bernilai 0 dan 1) dari sebuah gambar berukuran 30 x 30 pixel. Pada kasus gambar tersebut, kita dapat memiliki 900 unit input atau fitur yang merupakan setiap unit pixel yang dimiliki oleh gambar.

Gambar \@ref(fig:ANN1) menampilkan contoh ANN dengan 4 fitur, 2 hiden layer dan satu neuron output.

```{r ANN1,echo=FALSE, fig.cap='Contoh ANN dengan 4 fitur, 2 hiden layer dan satu neuron output', tidy=FALSE, out.width='80%', fig.align='center', message=FALSE, warning=FALSE}
library(knitr)
img1_path <- "./images/ANN1.png"
include_graphics(img1_path)
```

Gambar \@ref(fig:ANN2) menampilkan representasi sebuah proses transformasi data melalui sebuah neuron pada *hidden layer*.

```{r ANN2,echo=FALSE, fig.cap='Representasi sebuah neuron pada lapisan hidden layer', tidy=FALSE, out.width='80%', fig.align='center', message=FALSE, warning=FALSE}
library(knitr)
img1_path <- "./images/ANN2.png"
include_graphics(img1_path)
```

Seperti yang telah dijabarkan sebelumnya, pada proses propagasi maju, matks riinput data akan melalui proses perkalian dot terhadap matriks bobotnya (masing-masing input dikalikan dengan bobot dan dijumlahkan seluruhnya). Hasil yang diperoleh selanjutnya dijumlahkan dengan bias. Bias mirip dengan intersep yang ditambahkan dalam persamaan linear. Selain itu, bias merupakan parameter tambahan di ANN yang digunakan untuk menyesuaikan output bersama dengan jumlah bobot input ke neuron. Selain itu, nilai bias memungkinkan kita untuk menggeser fungsi aktivasi ke kanan atau kiri. Dengan demikian, Bias adalah konstanta yang membantu model untuk memproses input data dengan cara yang paling sesuai untuk data yang diberikan. Hasil transformasi data selanjutnya menjadi input bagi *layer* selanjutnya.

Pada tahapan akhir proses propagasi maju, fungsi *cost* dihitung dengan membandingkan nilai hasil prediksi dengan nilai aktual serta menghitung nilai jarak antara kedua nilai tersebut. Fungsi *cost* berguna untuk melihat sejauh mana model ANN menangkap pola dalam data.

### Matriks Input {#inputmat}

Misalkan terdapat sebuah persamaan linier dengan jumlah fitur $n$ :

\begin{equation}
y^{\left(i\right)}=w_0+w_1x_1^{\left(i\right)}+w_2x_2^{\left(i\right)}+\cdots+w_nx_n^{\left(i\right)}+\epsilon_i^{\left(i\right)}\ \ untuk\ i=1,\dots,m
  (\#eq:lineq)
\end{equation}

Kita dapat merepresentasikan Persamaan \@ref(eq:lineq) ke dalam notasi matriks yang ditunjukkan pada Persamaan \@ref(eq:lineq2) 


\begin{equation}
\begin{split}
\begin{bmatrix}
y_1 \\[0.3em]
y_2 \\[0.3em]
\vdots \\[0.3em] 
y_m 
\end{bmatrix} & = \begin{bmatrix}
       1       & x_{1}^{\left(1\right)} & x_{1}^{\left(1\right)} & \cdots & x_{n}^{\left(1\right)}           \\[0.3em]
       1       & x_{1}^{\left(2\right)} & x_{1}^{\left(2\right)} & \cdots & x_{n}^{\left(2\right)}           \\[0.3em]
       \vdots  & \vdots                 & \vdots                 & \ddots & \vdots                           \\[0.3em]
       1       & x_{1}^{\left(m\right)} & x_{1}^{\left(m\right)} & \cdots & x_{n}^{\left(m\right)}           \\[0.3em]
     \end{bmatrix}
     
\begin{bmatrix}
w_1 \\[0.3em]
w_2 \\[0.3em]
\vdots \\[0.3em] 
w_m
\end{bmatrix}
     
+ \begin{bmatrix}
\epsilon_1 \\[0.3em]
\epsilon_2 \\[0.3em]
\vdots \\[0.3em] 
\epsilon_m
\end{bmatrix} \\
& = \begin{bmatrix}
w_1 & w_2 & \cdots & w_m 
\end{bmatrix}

\begin{bmatrix}
       1                            & 1                      &  \cdots                & 1                                         \\[0.3em]
       x_{1}^{\left(2\right)}       & x_{1}^{\left(2\right)} & \cdots                 & x_{n}^{\left(m\right)}           \\[0.3em]
       \vdots                       & \vdots                 & \ddots                 & \vdots                                    \\[0.3em]
       x_{n}^{\left(2\right)}       & x_{n}^{\left(2\right)} & \cdots                 & x_{n}^{\left(m\right)}           \\[0.3em]
     \end{bmatrix}
     
+ \begin{bmatrix}
\epsilon_1 \\[0.3em]
\epsilon_2 \\[0.3em]
\vdots \\[0.3em] 
\epsilon_m
\end{bmatrix}
\end{split}
  (\#eq:lineq2)
\end{equation}

Jika error $\epsilon$ pada Persamaan \@ref(eq:lineq) dan Persamaan \@ref(eq:lineq2) diabaikan maka persamaan yang terbentuk adalah Persamaan \@ref(eq:lineq3).

\begin{equation}
Y = W^{T} X
  (\#eq:lineq3)
\end{equation}

Dimensi matriks input berdasarkan Persamaan \@ref(eq:lineq3) adalah **(_jumlah fitur_, _jumlah observasi_)**. Misalkan terdapat sebuah model ANN seperti yang ditunjukkan pada Gambar \@ref(fig:ANN1) dengan 4 fitur dan 2 observasi, maka matriks input pada model tersebut dapat dituliskan sebagai berikut:

\begin{equation}
X = \begin{bmatrix}
X_1 & X_2
\end{bmatrix}
= \begin{bmatrix}
x_{1}^{\left(1\right)} & x_{1}^{\left(2\right)} \\[0.3em]
x_{2}^{\left(1\right)} & x_{2}^{\left(2\right)} \\[0.3em]
x_{3}^{\left(1\right)} & x_{3}^{\left(2\right)} \\[0.3em]
x_{4}^{\left(1\right)} & x_{4}^{\left(2\right)} \\[0.3em]
\end{bmatrix}
\end{equation}

### Matriks Bias {#biasmat}

Secara umum dimensi matriks bias adalah **(_jumlah node pada layer $z^{\left[l\right]}$_, _1_)**. Pada Gambar \@ref(fig:ANN1), dimensi matriks bias *hidden layer* 1 adalah (3,1) karena terdapat 3 buah node pada *layer* tersebut. Matriks bias pada *hidden layer* 1 dapat dituliskan sebagai berikut:

\begin{equation}
b^{\left[1\right]} = \begin{bmatrix}
r \\[0.3em]
s \\[0.3em]
t
\end{bmatrix}
\end{equation}

Dimensi matriks bias pada *hidden layer* 2 dapat dituliskan sebagai berikut;

\begin{equation}
b^{\left[1\right]} = \begin{bmatrix}
u \\[0.3em]
v \\[0.3em]
w
\end{bmatrix}
\end{equation}

### Matriks Bobot {#weightmat}

Dimensi matriks bobot ditentukan oleh jumlah node pada layer awal dan pada layer selanjutnya. Sebagai Contoh pada Gambar \@ref(fig:ANN1), matriks bobot dari *input layer* menuju *hidden layer* 1 memiliki dimensi (4,2), matriks bobot dari *hidden layer* 1 menuju *hidden layer* 2 memiliki dimensi (3,3), dan matriks bobot dari *hidden layer* 2 menuju *output layer* memiliki dimensi (3,1). Berdasarkan contoh tersebut, dimensi matriks bobot dapat digeneralisasi menjadi **(_jumlah node ke-(l-1)_, _jumlah node ke-l_)**. 

### Fungsi Aktivasi {#active}

Setelah dilakukan penambahan bias pada hasil perkalian titik, generalisasi proses perkalian titik input pada tiap *hidden layer* dapat dinyatakan melalui Persamaan \@ref(eq:active1):

\begin{equation}
Z^{\left[l\right]}=W^{\left[l\right]T} A^{\left[l-1\right]} + b^{\left[l\right]}
  (\#eq:active1)
\end{equation}

Sebagai contoh perkalian titik pada *hidden layer* 1 berdasarkan Gambar \@ref(fig:ANN1) dapat dituliskan sebagai berikut:

\begin{equation}
Z^{\left[1\right]}=W^{\left[1\right]T} X + b^{\left[1\right]}
  (\#eq:active1)
\end{equation}

dimana $X$ merupakan input pada *hidden layer* 1, dan $Z^{\left[l\right]}$ merupakan input menuju *hidden layer* selanjutnya.

Secara umum $Z^{\left[l\right]}$ memiliki dimensi **(_jumlah node pada hidden layer l_, _jumlah observasi_)**. Pada contoh input pada Gambar \@ref(fig:ANN1), $Z^{\left[1\right]}$ memiliki dimensi (3,2).

Persamaan $Z^{\left[1\right]}$ dapat dituliskan ke dalam notasi matriks berikut:

\begin{equation}
\begin{split}
Z^{\left[1\right]} & =

\begin{bmatrix}
w_{11} & w_{12} & w_{13} \\[0.3em]
w_{21} & w_{22} & w_{23} \\[0.3em]
w_{31} & w_{32} & w_{33} \\[0.3em]
w_{41} & w_{42} & w_{43}
\end{bmatrix}

\begin{bmatrix}
x_{1}^{\left(1\right)} & x_{1}^{\left(2\right)} \\[0.3em]
x_{2}^{\left(1\right)} & x_{2}^{\left(2\right)} \\[0.3em]
x_{3}^{\left(1\right)} & x_{3}^{\left(2\right)} \\[0.3em]
x_{4}^{\left(1\right)} & x_{4}^{\left(2\right)} \\[0.3em]
\end{bmatrix}

+

\begin{bmatrix}
r \\[0.3em]
s \\[0.3em]
t
\end{bmatrix}

& = \begin{bmatrix}
z_{1,1}^{\left[1\right]} & z_{1,2}^{\left[1\right]} \\[0.3em]
z_{2,1}^{\left[1\right]} & z_{2,2}^{\left[1\right]} \\[0.3em]
z_{3,1}^{\left[1\right]} & z_{3,2}^{\left[1\right]} \\[0.3em]
\end{bmatrix}

\end{split}
\end{equation}

Fungsi aktivasi $A^{\left[1\right]}$ memiliki dimensi yang sama dengan $Z^{\left[1\right]}$ dan dituliskan ke dalam notasi matriks berikut:

\begin{equation}
\begin{bmatrix}
g\left(z_{1,1}^{\left[1\right]}\right) & g\left(z_{1,2}^{\left[1\right]}\right) \\[0.3em]
g\left(z_{2,1}^{\left[1\right]}\right) & g\left(z_{2,2}^{\left[1\right]}\right) \\[0.3em]
g\left(z_{3,1}^{\left[1\right]}\right) & g\left(z_{3,2}^{\left[1\right]}\right) \\[0.3em]
\end{bmatrix}
\end{equation}

dimana $g$ merupakan sebuah fungsi aktivasi (contoh: sigmoid, linier, dll.).

Matriks $A^{\left[1\right]}$ selanjutnya menjadi matriks input pada *hidden layer* 2. Perkalian titik pada *hidden layer* 2 dapat dituliskan ke dalam persamaan berikut:

\begin{equation}
\begin{split}
Z^{\left[2\right]} & = W^{\left[2\right]T} A^{\left[1\right]} + b^{\left[2\right]}

& = \begin{bmatrix}
w_{11} & w_{12} & w_{13} \\[0.3em]
w_{21} & w_{22} & w_{23} \\[0.3em]
w_{31} & w_{32} & w_{33} 
\end{bmatrix}

\begin{bmatrix}
g\left(z_{1,1}^{\left[1\right]}\right) & g\left(z_{1,2}^{\left[1\right]}\right) \\[0.3em]
g\left(z_{2,1}^{\left[1\right]}\right) & g\left(z_{2,2}^{\left[1\right]}\right) \\[0.3em]
g\left(z_{3,1}^{\left[1\right]}\right) & g\left(z_{3,2}^{\left[1\right]}\right) \\[0.3em]
\end{bmatrix}

+ bias^{\left[2\right]}

\end{split}
\end{equation}

Fungsi aktivasi berdasarkan input $Z^{\left[2\right]}$ adalah sebagai berikut:

\begin{equation}
A^{\left[2\right]} =
\begin{bmatrix}
g\left(z_{1,1}^{\left[2\right]}\right) & g\left(z_{1,2}^{\left[2\right]}\right) \\[0.3em]
g\left(z_{2,1}^{\left[2\right]}\right) & g\left(z_{2,2}^{\left[2\right]}\right) \\[0.3em]
g\left(z_{3,1}^{\left[2\right]}\right) & g\left(z_{3,2}^{\left[2\right]}\right) \\[0.3em]
\end{bmatrix}
\end{equation}

Layer terakhir pada skema model yang ditunjukkan pada Gambar \@ref(fig:ANN1) adalah *output layer*. Perkalian titik input pada *layer* tersebut adalah sebagai berikut:

\begin{equation}
\begin{split}
Z^{\left[3\right]} & = W^{\left[3\right]T} A^{\left[2\right]} + b^{\left[3\right]}

& = \begin{bmatrix}
w_{11}  \\[0.3em]
w_{21}  \\[0.3em]
w_{31}  
\end{bmatrix}

\begin{bmatrix}
g\left(z_{1,1}^{\left[2\right]}\right) & g\left(z_{1,2}^{\left[2\right]}\right) \\[0.3em]
g\left(z_{2,1}^{\left[2\right]}\right) & g\left(z_{2,2}^{\left[2\right]}\right) \\[0.3em]
g\left(z_{3,1}^{\left[2\right]}\right) & g\left(z_{3,2}^{\left[2\right]}\right) \\[0.3em]
\end{bmatrix}

+ bias^{\left[3\right]}

\end{split}
\end{equation}

Berdasarkan $Z^{\left[3\right]}$, dimensi A^{\left[3\right]} adalah (1, 3).

\begin{equation}
A^{\left[3\right]} =
\begin{bmatrix}
g\left(z_{1,1}^{\left[3\right]}\right) & g\left(z_{1,2}^{\left[3\right]}\right)
\end{bmatrix}
\end{equation}


### *Output Layer* {#outlayer}

Berdasarkan hasil pembahasan sebelumnya, output dari model diperoleh melalui matriks A^{\left[3\right]}. Pada kasus klasifikasi biner, nilai tiap elemen merupakan nilai probabilitas. Pelabelan output selanjutnya dilakukan dengan menetapkan sejumlah batas, seperti: jika $\ge 0,5$ maka labelnya adalah 1, dan jika $ < 0,5$ maka labelnya adalah 0.

### Ringkasan Propagasi Maju










