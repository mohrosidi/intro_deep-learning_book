[
["index.html", "Pengenalan Deep Learning Pengantar", " Pengenalan Deep Learning Mohammad Rosidi 2020-03-05 Pengantar body{ text-align: justify} "],
["intro.html", "Chapter 1 Pengantar Deep Learning", " Chapter 1 Pengantar Deep Learning Proses pegerjaan "],
["DL.html", "Chapter 2 Konsep Proses Pembelajaran Deep Learning", " Chapter 2 Konsep Proses Pembelajaran Deep Learning body{ text-align: justify} Proses pembelajaran Artificial Neural Network (ANN) pada dasarnya digunakan untuk mencari nilai bobot (\\(w\\)) optimal yang menghubungkan antar lapisan neuron sehingga diperoleh cost atau kesalahan minimum. Proses pembelajaran pada ANN dilakukan melalui dua proses utama, antara lain: Propagasi maju (forward propagation) Propagasi mundur (bacward propagation) Secara umum propagasi maju proses dimana ANN memproses input data untuk melakukan prediksi terhadap nilai variabel target. Tahapan ini melibatkan proses perkalian titik antara input dan bobot dan pemjumlahan dengan bias dari masing-masing layer ANN. Hasil penjumlahan selanjutnya transformasi melalui fungsi aktivasi. Pada awal proses pembelajaran, bobot dat bias pada tiap jaringan ditetapkan secara acak, sehingga jaringan mengimplementasikan serangkaian transformasi secara acak. Hal ini menyebabkan nilai output yang dihasilkan jauh dari nilai yang diharapkan atau cost yang dihasilkan sangat tinggi. Untuk mengatasi cost yang tinggi tersebut, bobot dilakukan penyesuaian sehingga pada proses pembelajaran selanjutnya nilai cost yang dihasilkan menurun. Proses tersebut diulangi secara terus-menerus hingga diperoleh cost minimum yang konvergen. Tahapan tersebut disebut sebagai tahapan propagasi balik. Pada tahapan ini, hasil prediksi dari proses propagasi balik dihitung nilai cost-nya (error). Nilai tersebut selanjutnya dijadikan sebagai dasar dalam melakukan penyesuaian bobot dan bias pada ANN. Gambaran proses pembelajaran ditampilkan pada Gambar 2.1. Gambar 2.1: Proses pembelajaran ANN. (Sumber:Chollet, 2018) "],
["notation.html", "2.1 Notasi Matematika", " 2.1 Notasi Matematika body{ text-align: justify} Terdapat sejumlah notasi matematika yang akan digunakan dalam pembahasan Chapter ini. Notasi matematika tersebut antara lain: Untuk input \\(x \\in R^{n_x}\\) (\\(n\\) variabel atau fitur) superskrip (\\(i\\)): menunjukkan referensi observasi ke-\\(i\\), contoh: \\(x^{\\left(2\\right)}\\) merupakan nilai input pada observasi atau baris kedua. subskrip \\(n\\) : menunjukkan referensi fitur atau variabel ke-\\(n\\), contoh: \\(x^{\\left(2\\right)}_{1}\\) merupakan observasi kedua pada fitur pertama. Untuk network layer: superskrip [\\(I\\)]: menunjukkan referensi layer ke-\\(I\\), contoh: \\(z^{\\left[2\\right]}\\) merupakan fungsi aktivasi pada layer 2. subskrip \\(n\\) : menunjukkan referensi node atau neuron ke-\\(n\\), contoh: \\(z^{\\left[2\\right]}_{1}\\) merupakan fungsi aktivasi ditambah bias pada node 1 pada layer 2. "],
["forwardprop.html", "2.2 Konsep Propagasi Maju", " 2.2 Konsep Propagasi Maju body{ text-align: justify} Input sebuah model ANN dapat berupa apapun. Input dapat berupa intensitas abu-abu (bernilai 0 dan 1) dari sebuah gambar berukuran 30 x 30 pixel. Pada kasus gambar tersebut, kita dapat memiliki 900 unit input atau fitur yang merupakan setiap unit pixel yang dimiliki oleh gambar. Gambar 2.2 menampilkan contoh ANN dengan 4 fitur, 2 hiden layer dan satu neuron output. Gambar 2.2: Contoh ANN dengan 4 fitur, 2 hiden layer dan satu neuron output Gambar 2.3 menampilkan representasi sebuah proses transformasi data melalui sebuah neuron pada hidden layer. Gambar 2.3: Representasi sebuah neuron pada lapisan hidden layer Seperti yang telah dijabarkan sebelumnya, pada proses propagasi maju, matks riinput data akan melalui proses perkalian dot terhadap matriks bobotnya (masing-masing input dikalikan dengan bobot dan dijumlahkan seluruhnya). Hasil yang diperoleh selanjutnya dijumlahkan dengan bias. Bias mirip dengan intersep yang ditambahkan dalam persamaan linear. Selain itu, bias merupakan parameter tambahan di ANN yang digunakan untuk menyesuaikan output bersama dengan jumlah bobot input ke neuron. Selain itu, nilai bias memungkinkan kita untuk menggeser fungsi aktivasi ke kanan atau kiri. Dengan demikian, Bias adalah konstanta yang membantu model untuk memproses input data dengan cara yang paling sesuai untuk data yang diberikan. Hasil transformasi data selanjutnya menjadi input bagi layer selanjutnya. Pada tahapan akhir proses propagasi maju, fungsi cost dihitung dengan membandingkan nilai hasil prediksi dengan nilai aktual serta menghitung nilai jarak antara kedua nilai tersebut. Fungsi cost berguna untuk melihat sejauh mana model ANN menangkap pola dalam data. 2.2.1 Matriks Input Misalkan terdapat sebuah persamaan linier dengan jumlah fitur \\(n\\) : \\[\\begin{equation} y^{\\left(i\\right)}=w_0+w_1x_1^{\\left(i\\right)}+w_2x_2^{\\left(i\\right)}+\\cdots+w_nx_n^{\\left(i\\right)}+\\epsilon_i^{\\left(i\\right)}\\ \\ untuk\\ i=1,\\dots,m \\tag{2.1} \\end{equation}\\] Kita dapat merepresentasikan Persamaan (2.1) ke dalam notasi matriks yang ditunjukkan pada Persamaan (2.2). \\[\\begin{equation} \\begin{split} \\begin{bmatrix} y_1 \\\\[0.3em] y_2 \\\\[0.3em] \\vdots \\\\[0.3em] y_m \\end{bmatrix} &amp; = \\begin{bmatrix} 1 &amp; x_{1}^{\\left(1\\right)} &amp; x_{1}^{\\left(1\\right)} &amp; \\cdots &amp; x_{n}^{\\left(1\\right)} \\\\[0.3em] 1 &amp; x_{1}^{\\left(2\\right)} &amp; x_{1}^{\\left(2\\right)} &amp; \\cdots &amp; x_{n}^{\\left(2\\right)} \\\\[0.3em] \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\[0.3em] 1 &amp; x_{1}^{\\left(m\\right)} &amp; x_{1}^{\\left(m\\right)} &amp; \\cdots &amp; x_{n}^{\\left(m\\right)} \\\\[0.3em] \\end{bmatrix} \\begin{bmatrix} w_1 \\\\[0.3em] w_2 \\\\[0.3em] \\vdots \\\\[0.3em] w_m \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\[0.3em] \\epsilon_2 \\\\[0.3em] \\vdots \\\\[0.3em] \\epsilon_m \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} w_1 &amp; w_2 &amp; \\cdots &amp; w_m \\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1 \\\\[0.3em] x_{1}^{\\left(2\\right)} &amp; x_{1}^{\\left(2\\right)} &amp; \\cdots &amp; x_{n}^{\\left(m\\right)} \\\\[0.3em] \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\[0.3em] x_{n}^{\\left(2\\right)} &amp; x_{n}^{\\left(2\\right)} &amp; \\cdots &amp; x_{n}^{\\left(m\\right)} \\\\[0.3em] \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\[0.3em] \\epsilon_2 \\\\[0.3em] \\vdots \\\\[0.3em] \\epsilon_m \\end{bmatrix} \\end{split} \\tag{2.2} \\end{equation}\\] Jika error \\(\\epsilon\\) pada Persamaan (2.1) dan Persamaan (2.2) diabaikan maka persamaan yang terbentuk adalah Persamaan (2.3). \\[\\begin{equation} Y = W^{T} X \\tag{2.3} \\end{equation}\\] Dimensi matriks input berdasarkan Persamaan (2.3) adalah (jumlah fitur, jumlah observasi). Misalkan terdapat sebuah model ANN seperti yang ditunjukkan pada Gambar 2.2 dengan 4 fitur dan 2 observasi, maka matriks input pada model tersebut dapat dituliskan sebagai berikut: \\[\\begin{equation} X = \\begin{bmatrix} X_1 &amp; X_2 \\end{bmatrix} = \\begin{bmatrix} x_{1}^{\\left(1\\right)} &amp; x_{1}^{\\left(2\\right)} \\\\[0.3em] x_{2}^{\\left(1\\right)} &amp; x_{2}^{\\left(2\\right)} \\\\[0.3em] x_{3}^{\\left(1\\right)} &amp; x_{3}^{\\left(2\\right)} \\\\[0.3em] x_{4}^{\\left(1\\right)} &amp; x_{4}^{\\left(2\\right)} \\\\[0.3em] \\end{bmatrix} \\end{equation}\\] 2.2.2 Matriks Bias Secara umum dimensi matriks bias adalah (jumlah node pada layer \\(z^{\\left[l\\right]}\\), 1). Pada Gambar 2.2, dimensi matriks bias hidden layer 1 adalah (3,1) karena terdapat 3 buah node pada layer tersebut. Matriks bias pada hidden layer 1 dapat dituliskan sebagai berikut: \\[\\begin{equation} b^{\\left[1\\right]} = \\begin{bmatrix} r \\\\[0.3em] s \\\\[0.3em] t \\end{bmatrix} \\end{equation}\\] Dimensi matriks bias pada hidden layer 2 dapat dituliskan sebagai berikut; \\[\\begin{equation} b^{\\left[1\\right]} = \\begin{bmatrix} u \\\\[0.3em] v \\\\[0.3em] w \\end{bmatrix} \\end{equation}\\] 2.2.3 Matriks Bobot Dimensi matriks bobot ditentukan oleh jumlah node pada layer awal dan pada layer selanjutnya. Sebagai Contoh pada Gambar 2.2, matriks bobot dari input layer menuju hidden layer 1 memiliki dimensi (4,2), matriks bobot dari hidden layer 1 menuju hidden layer 2 memiliki dimensi (3,3), dan matriks bobot dari hidden layer 2 menuju output layer memiliki dimensi (3,1). Berdasarkan contoh tersebut, dimensi matriks bobot dapat digeneralisasi menjadi (jumlah node ke-(l-1), jumlah node ke-l). 2.2.4 Fungsi Aktivasi Setelah dilakukan penambahan bias pada hasil perkalian titik, generalisasi proses perkalian titik input pada tiap hidden layer dapat dinyatakan melalui Persamaan (2.4): \\[\\begin{equation} Z^{\\left[l\\right]}=W^{\\left[l\\right]T} A^{\\left[l-1\\right]} + b^{\\left[l\\right]} \\tag{2.4} \\end{equation}\\] Sebagai contoh perkalian titik pada hidden layer 1 berdasarkan Gambar 2.2 dapat dituliskan sebagai berikut: \\[\\begin{equation} Z^{\\left[1\\right]}=W^{\\left[1\\right]T} X + b^{\\left[1\\right]} \\tag{2.4} \\end{equation}\\] dimana \\(X\\) merupakan input pada hidden layer 1, dan \\(Z^{\\left[l\\right]}\\) merupakan input menuju hidden layer selanjutnya. Secara umum \\(Z^{\\left[l\\right]}\\) memiliki dimensi (jumlah node pada hidden layer l, jumlah observasi). Pada contoh input pada Gambar 2.2, \\(Z^{\\left[1\\right]}\\) memiliki dimensi (3,2). Persamaan \\(Z^{\\left[1\\right]}\\) dapat dituliskan ke dalam notasi matriks berikut: \\[\\begin{equation} \\begin{split} Z^{\\left[1\\right]} &amp; = \\begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13} \\\\[0.3em] w_{21} &amp; w_{22} &amp; w_{23} \\\\[0.3em] w_{31} &amp; w_{32} &amp; w_{33} \\\\[0.3em] w_{41} &amp; w_{42} &amp; w_{43} \\end{bmatrix} \\begin{bmatrix} x_{1}^{\\left(1\\right)} &amp; x_{1}^{\\left(2\\right)} \\\\[0.3em] x_{2}^{\\left(1\\right)} &amp; x_{2}^{\\left(2\\right)} \\\\[0.3em] x_{3}^{\\left(1\\right)} &amp; x_{3}^{\\left(2\\right)} \\\\[0.3em] x_{4}^{\\left(1\\right)} &amp; x_{4}^{\\left(2\\right)} \\\\[0.3em] \\end{bmatrix} + \\begin{bmatrix} r \\\\[0.3em] s \\\\[0.3em] t \\end{bmatrix} &amp; = \\begin{bmatrix} z_{1,1}^{\\left[1\\right]} &amp; z_{1,2}^{\\left[1\\right]} \\\\[0.3em] z_{2,1}^{\\left[1\\right]} &amp; z_{2,2}^{\\left[1\\right]} \\\\[0.3em] z_{3,1}^{\\left[1\\right]} &amp; z_{3,2}^{\\left[1\\right]} \\\\[0.3em] \\end{bmatrix} \\end{split} \\end{equation}\\] Fungsi aktivasi \\(A^{\\left[1\\right]}\\) memiliki dimensi yang sama dengan \\(Z^{\\left[1\\right]}\\) dan dituliskan ke dalam notasi matriks berikut: \\[\\begin{equation} \\begin{bmatrix} g\\left(z_{1,1}^{\\left[1\\right]}\\right) &amp; g\\left(z_{1,2}^{\\left[1\\right]}\\right) \\\\[0.3em] g\\left(z_{2,1}^{\\left[1\\right]}\\right) &amp; g\\left(z_{2,2}^{\\left[1\\right]}\\right) \\\\[0.3em] g\\left(z_{3,1}^{\\left[1\\right]}\\right) &amp; g\\left(z_{3,2}^{\\left[1\\right]}\\right) \\\\[0.3em] \\end{bmatrix} \\end{equation}\\] dimana \\(g\\) merupakan sebuah fungsi aktivasi (contoh: sigmoid, linier, dll.). Matriks \\(A^{\\left[1\\right]}\\) selanjutnya menjadi matriks input pada hidden layer 2. Perkalian titik pada hidden layer 2 dapat dituliskan ke dalam persamaan berikut: \\[\\begin{equation} \\begin{split} Z^{\\left[2\\right]} &amp; = W^{\\left[2\\right]T} A^{\\left[1\\right]} + b^{\\left[2\\right]} &amp; = \\begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13} \\\\[0.3em] w_{21} &amp; w_{22} &amp; w_{23} \\\\[0.3em] w_{31} &amp; w_{32} &amp; w_{33} \\end{bmatrix} \\begin{bmatrix} g\\left(z_{1,1}^{\\left[1\\right]}\\right) &amp; g\\left(z_{1,2}^{\\left[1\\right]}\\right) \\\\[0.3em] g\\left(z_{2,1}^{\\left[1\\right]}\\right) &amp; g\\left(z_{2,2}^{\\left[1\\right]}\\right) \\\\[0.3em] g\\left(z_{3,1}^{\\left[1\\right]}\\right) &amp; g\\left(z_{3,2}^{\\left[1\\right]}\\right) \\\\[0.3em] \\end{bmatrix} + bias^{\\left[2\\right]} \\end{split} \\end{equation}\\] Fungsi aktivasi berdasarkan input \\(Z^{\\left[2\\right]}\\) adalah sebagai berikut: \\[\\begin{equation} A^{\\left[2\\right]} = \\begin{bmatrix} g\\left(z_{1,1}^{\\left[2\\right]}\\right) &amp; g\\left(z_{1,2}^{\\left[2\\right]}\\right) \\\\[0.3em] g\\left(z_{2,1}^{\\left[2\\right]}\\right) &amp; g\\left(z_{2,2}^{\\left[2\\right]}\\right) \\\\[0.3em] g\\left(z_{3,1}^{\\left[2\\right]}\\right) &amp; g\\left(z_{3,2}^{\\left[2\\right]}\\right) \\\\[0.3em] \\end{bmatrix} \\end{equation}\\] Layer terakhir pada skema model yang ditunjukkan pada Gambar 2.2 adalah output layer. Perkalian titik input pada layer tersebut adalah sebagai berikut: \\[\\begin{equation} \\begin{split} Z^{\\left[3\\right]} &amp; = W^{\\left[3\\right]T} A^{\\left[2\\right]} + b^{\\left[3\\right]} &amp; = \\begin{bmatrix} w_{11} \\\\[0.3em] w_{21} \\\\[0.3em] w_{31} \\end{bmatrix} \\begin{bmatrix} g\\left(z_{1,1}^{\\left[2\\right]}\\right) &amp; g\\left(z_{1,2}^{\\left[2\\right]}\\right) \\\\[0.3em] g\\left(z_{2,1}^{\\left[2\\right]}\\right) &amp; g\\left(z_{2,2}^{\\left[2\\right]}\\right) \\\\[0.3em] g\\left(z_{3,1}^{\\left[2\\right]}\\right) &amp; g\\left(z_{3,2}^{\\left[2\\right]}\\right) \\\\[0.3em] \\end{bmatrix} + bias^{\\left[3\\right]} \\end{split} \\end{equation}\\] Berdasarkan \\(Z^{\\left[3\\right]}\\), dimensi A^{} adalah (1, 3). \\[\\begin{equation} A^{\\left[3\\right]} = \\begin{bmatrix} g\\left(z_{1,1}^{\\left[3\\right]}\\right) &amp; g\\left(z_{1,2}^{\\left[3\\right]}\\right) \\end{bmatrix} \\end{equation}\\] 2.2.5 Output Layer Berdasarkan hasil pembahasan sebelumnya, output dari model diperoleh melalui matriks A^{}. Pada kasus klasifikasi biner, nilai tiap elemen merupakan nilai probabilitas. Pelabelan output selanjutnya dilakukan dengan menetapkan sejumlah batas, seperti: jika \\(\\ge 0,5\\) maka labelnya adalah 1, dan jika $ &lt; 0,5$ maka labelnya adalah 0. 2.2.6 Ringkasan Propagasi Maju Propagasi maju untuk model ANN dengan 3 layer dan sebuah observasi dapat diringkas menjadi persamaan berikut: \\[\\begin{equation} \\begin{matrix} z^{\\left[1\\right]\\left(i\\right)}=w^{\\left[1\\right]}x^{\\left(i\\right)}+b^{\\left[1\\right]\\left(i\\right)} \\\\[0.3em] a^{\\left[1\\right]\\left(i\\right)}=g^{\\left[1\\right]}\\left(z^{\\left[1\\right]\\left(i\\right)}\\right) \\\\[0.3em] z^{\\left[2\\right]\\left(i\\right)}=w^{\\left[2\\right]}a^{\\left[1\\right]\\left(i\\right)}+b^{\\left[2\\right]\\left(i\\right)} \\\\[0.3em] a^{\\left[2\\right]\\left(i\\right)}=g^{\\left[2\\right]}\\left(z^{\\left[2\\right]\\left(i\\right)}\\right) \\\\[0.3em] z^{\\left[3\\right]\\left(i\\right)}=w^{\\left[3\\right]}a^{\\left[2\\right]\\left(i\\right)}+b^{\\left[3\\right]\\left(i\\right)} \\\\[0.3em] a^{\\left[3\\right]\\left(i\\right)}=g^{\\left[3\\right]}\\left(z^{\\left[3\\right]\\left(i\\right)}\\right) \\end{matrix} \\end{equation}\\] Bentuk tervektorisasi dari persamaan tersebut ditampilkan ke dalam Persamaan (2.5). \\[\\begin{equation} \\begin{matrix} untuk\\ l\\ =\\ 1,2,3,\\dots,L \\\\[0.3em] Z^{\\left[l\\right]}=W^{\\left[l\\right]}A^{\\left[l\\right]}+b^{\\left[l\\right]} \\\\[0.3em] A^{\\left[l\\right]}=g^{\\left[l\\right]}\\left(Z^{\\left[l\\right]}\\right) \\end{matrix} \\tag{2.5} \\end{equation}\\] Dimensi matriks untuk setiap layer ANN diringkas ke dalam Tabel 2.1: Tabel 2.1: Dimensi matriks bobot. bias, dan fungsi aktivasi pada tiap layer ANN. Dim W Dim b Aktivasi Dim Aktivasi Layer 1 \\(\\left(jumlah\\ fitur,n^{\\left[1\\right]}\\right)\\) \\(\\left(n^{\\left[1\\right]},1\\right)\\) \\(Z^{\\left[1\\right]}=W^{\\left[1\\right]}X+b^{\\left[1\\right]}\\) \\(\\left(n^{\\left[1\\right]},jumlah\\ obs.\\right)\\) Layer l \\(\\left(n^{\\left[l-1\\right]},n^{\\left[l\\right]}\\right)\\) \\(\\left(n^{\\left[l\\right]},1\\right)\\) \\(Z^{\\left[l\\right]}=W^{\\left[l\\right]}A^{\\left[l-1\\right]}+b^{\\left[l\\right]}\\) \\(\\left(n^{\\left[l\\right]},jumlah\\ obs.\\right)\\) Layer L \\(\\left(n^{\\left[L-1\\right]},n^{\\left[L\\right]}\\right)\\) \\(\\left(n^{\\left[L\\right]},1\\right)\\) \\(Z^{\\left[L\\right]}=W^{\\left[L\\right]}A^{\\left[L-1\\right]}+b^{\\left[L\\right]}\\) \\(\\left(n^{\\left[L\\right]},jumlah\\ obs.\\right)\\) "],
["actfun.html", "2.3 Fungsi Aktivasi", " 2.3 Fungsi Aktivasi body{ text-align: justify} Fungsi aktivasi merupakan fungsi matematika yang berfungsi untuk mentransformasi input data dari layer sebelumnya menjadi sebuah nilai yang mendekati nilai variabel target. Dalam ANN terdapat dua buah jenis fungsi aktivasi, yaitu: Fungsi aktivasi linier Fungsi aktivasi linier berupa garis lurus (lihat Gambar 2.4), dimana output dari fungsi memiliki rentang dari \\(- \\infty\\) sampai dengan \\(+ \\infty\\). Fungsi aktivasi ini biasa digunakan jika kita ingin membuat model regresi menggunakan ANN. Kekurangan fungsi aktivasi ini adalah fungsi ini tidak banyak membantu menangani kompleksitas data dan parameter yang ada pada ANN. Gambar 2.4: Fungsi linier. Fungsi aktivasi non-linier Sebagian besar pola data yang ada di permasalahan di dunia nyata adalah pola non-linier, sehingga untuk dapat menangkap pola tersebut ke dalam ANN diperlukan fungsi aktivasi yang non-linier. Fungsi aktivasi tersebut haruslah dapat menerima input data pada rentang \\(- \\infty\\) sampai dengan \\(+ \\infty\\) dan mentransformasikannya ke dalam rentang output terbatas seperti: [0, 1] atau [-1,1], dll. Fungsi aktivasi non-linier yang sering digunakan dan akan dibahas lebih lanjut dalam buku ini, antara lain: sigmoid, tanh, ReLU, dan softmax. 2.3.1 Sigmoid Fungsi aktivasi sigmoid merupakan fungsi aktivasi yang menghasilkan kurva logistik (lihat Gambar 2.5) dengan rentang ouput nilai [0,1]. Fungsi ini sering digunakan pada model klasifikasi biner. Persamaan fungsi aktivasi sigmoid ditampilkan pada Persamaan (2.6). Gambar 2.5: Fungsi sigmoid. \\[\\begin{equation} \\sigma\\left(Z\\right)=\\frac{1}{1+\\exp^{-Z}} \\tag{2.6} \\end{equation}\\] Properti yang tidak diinginkan dari fungsi aktivasi sigmoid adalah fungsi ini memiliki pola jenuh (stagnan) di kedua ekor dengan nilai 0 atau 1. Ketika ini terjadi, gradien di wilayah ini hampir mendekati nol. Selama backpropagation, gradien lokal akan dikalikan dengan gradien fungsi aktivasi sigmoid dan jika gradien lokal sangat kecil, hal tersebut akan secara efektif membuat gradien “menghilang” dan tidak ada sinyal yang akan mengalir melalui neuron ke bobot (bobot tidak dapat diperbaharui). 2.3.2 Tangen Hiperbolik Fungsi tangen hiperbolik memiliki pola grafik yang sama dengan fungsi aktivasi sigmoid (lihat Gambar 2.6). Perbedaan antara keduanya adalah fungsi tangen hiperbolik memiliki rentang nilai output [-1,1]. Persamaan fungsi tangen hiperbolik ditampilkan pada Persamaan (2.7). Gambar 2.6: Fungsi tanh vs sigmoid. \\[\\begin{equation} g\\left(Z\\right)=\\frac{\\exp^Z-\\exp^{-Z}}{\\exp^Z+\\exp^{-Z}} \\tag{2.7} \\end{equation}\\] "],
["initialization.html", "Chapter 3 Inisialisasi Parameter ANN", " Chapter 3 Inisialisasi Parameter ANN "],
["optimization.html", "Chapter 4 Optimasi Model ANN", " Chapter 4 Optimasi Model ANN "],
["referensi.html", "Referensi", " Referensi body{ text-align: justify} Fox, J. 2005. The R Commander: A Basic-Statistics Graphical User Interface to R. Journal of Statistical Software.Vol:14(9), p:1-42. Fox, J. 2017. Using the R Commader: A Point-and-Click Interface for R. CRC Press. Fox, J. Valat, M.B. 2018. Getting Started With the R Commander.https://socialsciences.mcmaster.ca/jfox/Misc/Rcmdr/Getting-Started-with-the-Rcmdr.pdf. Gio, P.U. Irawan, D.E. 2016. Belajar Statistika dengan R (disertai beberapa contoh perhitungan manual). USU Press : Medan. Nguyen-Feng, V. Stellmack, M.A. 2016. A Guide to Data Aalysis in R Commander. University of Minnesota. Primartha, R. 2018. Belajar Machine Learning Teori dan Praktik. Penerbit Informatika : Bandung. Quick-R. Data Input. https://www.statmethods.net/input/index.html Quick-R. Data Management. https://www.statmethods.net/management/index.html Rosadi,D. 2011. Analisis Ekonometrika dan Runtun Waktu Terapan dengan R. Penerbit Andi: Yogyakarta. Rosadi,D. 2016. Analisis Statistika dengan R. Gadjah Mada University Press: Yogyakarta. Rosidi, M. 2019. Metode Numerik Menggunakan R Untuk Teknik Lingkungan. https://bookdown.org/moh_rosidi2610/Metode_Numerik/. STHDA. Importing Data Into R . http://www.sthda.com/english/wiki/importing-data-into-r STHDA. Exporting Data From R. http://www.sthda.com/english/wiki/exporting-data-from-r STDHA. Getting Help With Functions In R Programming. http://www.sthda.com/english/wiki/getting-help-with-functions-in-r-programming . Venables, W.N. Smith D.M. and R Core Team. 2018. An Introduction to R. R Manuals. Wickham, H. Grolemund G. 2016. R For Data Science: Import, Tidy, Transform, Visualize, And Model Data. O’Reilly Media, Inc. Widodo, B. Rachmawati, R.N. 2013. Pengantar Praktis Pemrograman R untuk Ilmu Komputer. Halaman Moeka Publishing : Jakarta. "]
]
